<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:hadoop="http://www.springframework.org/schema/hadoop" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:batch="http://www.springframework.org/schema/batch"
	xmlns:context="http://www.springframework.org/schema/context"
	xmlns:util="http://www.springframework.org/schema/util"
	xmlns:script="http://www.springframework.org/schema/integration/scripting"
	xmlns:jdbc="http://www.springframework.org/schema/jdbc"
	xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
		http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd
		http://www.springframework.org/schema/hadoop http://www.springframework.org/schema/hadoop/spring-hadoop.xsd
		http://www.springframework.org/schema/batch http://www.springframework.org/schema/batch/spring-batch.xsd
		http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd
		http://www.springframework.org/schema/integration/scripting http://www.springframework.org/schema/integration/scripting/spring-integration-scripting.xsd
		http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc.xsd">

    <hadoop:configuration register-url-handler="false" properties-location="file:${xd.home}/config/hadoop.properties"/>
 	
 	<!-- Datasource for SQLFire -->
	<bean id="dataSourceSqlFire" class="org.apache.commons.dbcp.BasicDataSource"
		destroy-method="close">
		<property name="driverClassName" value="com.vmware.sqlfire.jdbc.ClientDriver" />
		<property name="url" value="jdbc:sqlfire://${memUrl:pivhdsne}:1527" />
		<property name="username" value="${username:gpadmin}" />
		<property name="password" value="${password:password}" />
	</bean>
	<bean id="sqlFireTemplate" class="org.springframework.jdbc.core.JdbcTemplate">
		<property name="dataSource" ref="dataSourceSqlFire" />
	</bean>
	
	<!-- Datasource for HAWQ -->
	<bean id="dataSourceHawq" class="org.apache.commons.dbcp.BasicDataSource"
		destroy-method="close">
		<property name="driverClassName" value="org.postgresql.Driver" />
		<property name="url" value="jdbc:postgresql://${url:pivhdsne}:5432/gpadmin" />
		<property name="username" value="${username:gpadmin}" />
		<property name="password" value="${password:password}" />
	</bean>
	<bean id="hawqTemplate" class="org.springframework.jdbc.core.JdbcTemplate">
		<property name="dataSource" ref="dataSourceHawq"/>
	</bean>
	
	<!-- required since Hadoop Job is a class not an interface and we need to 
		use a Job with step scope to access #{jobParameters['...']} -->
	<bean class="org.springframework.batch.core.scope.StepScope">
		<property name="proxyTargetClass" value="true" />
	</bean>

	<!-- Batch Job Logic -->
	<batch:job id="job">
		<batch:step id="sqlfTeardown" next="sqlfCreate">
			<batch:tasklet ref="sqlfTeardownTasklet" />
		</batch:step>
		<batch:step id="sqlfCreate" next="hawqTeardown">
			<batch:tasklet ref="sqlfCreateTasklet" />
		</batch:step>
		<batch:step id="hawqTeardown" next="hawqCreate">
			<batch:tasklet ref="hawqTeardownTasklet" />
		</batch:step>
		<batch:step id="hawqCreate" next="hdfsArchive">
			<batch:tasklet ref="hawqCreateTasklet" />
		</batch:step>
		<batch:step id="hdfsArchive">
			<batch:tasklet ref="hdfsArchiveTasklet" />
		</batch:step>
	</batch:job>
	
	<!-- Batch job script tasklet references -->
	<hadoop:script-tasklet id="sqlfTeardownTasklet"
		script-ref="sqlfTeardownScript" scope="step" />
	<hadoop:script-tasklet id="sqlfCreateTasklet"
		script-ref="sqlfCreateScript" scope="step" />
	<hadoop:script-tasklet id="hawqTeardownTasklet"
		script-ref="hawqTeardownScript" scope="step" />
	<hadoop:script-tasklet id="hawqCreateTasklet"
		script-ref="hawqCreateScript" scope="step" />
	<hadoop:script-tasklet id="hdfsArchiveTasklet"
		script-ref="hdfsArchiveScript" scope="step" />

	<!-- Batch job script references -->
	<hadoop:script id="sqlfTeardownScript" language="groovy">
		<hadoop:property name="template" ref="sqlFireTemplate" />
		sql = "drop table app.realtime_orders"
		
		try {
			template.execute(sql)
		}
		catch(Exception e) {
			println e
		}
		println sql
	</hadoop:script>
	<hadoop:script id="hawqCreateScript" language="groovy">
		<hadoop:property name="template" ref="hawqTemplate" />
		try {
			sql = """CREATE EXTERNAL TABLE realtime_orders_pxf (customer_id int, order_id integer, order_amount numeric(10,2), store_id integer, num_items integer) LOCATION ('pxf://pivhdsne:50070/xd/order_stream/order_stream-*.txt?profile=HdfsTextSimple') FORMAT 'TEXT' (DELIMITER = '|', null='null')"""
			template.execute(sql)
			println sql
		
			sql2 = "CREATE TABLE realtime_orders_hawq as select * from realtime_orders_pxf"
			template.execute(sql2)
			println sql2
		}
		catch(Exception e) {
			println "External table and HAWQ table not created, probably because there are no HDFS files"
		}
	</hadoop:script>
	<hadoop:script id="hawqTeardownScript" language="groovy">
		<hadoop:property name="template" ref="hawqTemplate" />
		sql = "DROP TABLE if exists realtime_orders_hawq"
		template.execute(sql)
		println sql
		
		sql2 = "DROP EXTERNAL TABLE if exists realtime_orders_pxf"
		template.execute(sql2)
		println sql2
	</hadoop:script>
	<hadoop:script id="sqlfCreateScript" language="groovy">
		<hadoop:property name="template" ref="sqlFireTemplate" />
		sql = "CREATE TABLE APP.realtime_orders(customer_id INT NOT NULL, order_id INT NOT NULL, order_amount NUMERIC(10,2), store_id VARCHAR(8), num_items INT) REPLICATE"
		template.execute(sql)
		println sql
	</hadoop:script>
	<hadoop:script id="hdfsArchiveScript" language="groovy">
		if (!fsh.test("/xd/order_stream_archive/")) {
   			fsh.mkdir("/xd/order_stream_archive/");
   		}
		fsh.cp("/xd/order_stream/*.txt","/xd/order_stream_archive/")
		fsh.rm("/xd/order_stream/*.txt")
		
		println "hdfs live:" + fsh.ls("/xd/order_stream").toString()
		println "hdfs archive:" + fsh.ls("/xd/order_stream_archive").toString()
	</hadoop:script>
</beans>

